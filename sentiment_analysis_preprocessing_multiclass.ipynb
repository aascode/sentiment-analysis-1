{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis preprocessing multiclass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TukimOu3IeLK",
        "colab_type": "text"
      },
      "source": [
        "## some documentation to check\n",
        "- process slang:\n",
        "    * https://github.com/vi3k6i5/flashtext1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ti03vHymBWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels\n",
        "highlights = {\n",
        "    # related with speech recognition\n",
        "    'professional qualities': ['handles pressure'],\n",
        "    'soft skills': ['silence'],\n",
        "    'answer analysis': ['filler words', 'long pause', 'focus', 'patience'], \n",
        "\n",
        "    'polarities': {\n",
        "        'negative': [\n",
        "                     # confidence\n",
        "                     'not confident', \n",
        "                     'unsure',\n",
        "\n",
        "                     # professional qualities\n",
        "                     '',\n",
        "                     'disordered',\n",
        "                     'talkative',\n",
        "                     'uninsterested', # 'engaged'\n",
        "\n",
        "                     # soft skills\n",
        "                     'sad',\n",
        "                     'unfriendly'\n",
        "                     ],\n",
        "\n",
        "        'positive': [\n",
        "                     # confidence\n",
        "                     'confident', \n",
        "                     'certany',\n",
        "\n",
        "                     # professional qualities\n",
        "                     'handles pressure',\n",
        "                     'organized',\n",
        "                     'concise', \n",
        "                     'interested', # 'engaged'\n",
        "\n",
        "                     # soft skills\n",
        "                     'happy',\n",
        "                     'friendly'\n",
        "                     ]\n",
        "    }\n",
        "}\n",
        "\n",
        "main_lst = list(highlights.values())\n",
        "main_labels = [k for j in main_lst for k in j]\n",
        "\n",
        "neg_pos_lst = highlights['polarities'].values()\n",
        "neg_pos_labels = [k for j in neg_pos_lst for k in j]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIuwykirmIbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "803942a4-1344-4e5d-9cfd-0397a2e0af3c"
      },
      "source": [
        "neg_pos_lst"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([['not confident', 'unsure', '', 'disordered', 'talkative', 'uninsterested', 'sad', 'unfriendly'], ['confident', 'certany', 'handles pressure', 'organized', 'concise', 'interested', 'happy', 'friendly']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug78ZM2qmJOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "d2a93407-cef3-490a-cf6d-94bf9cd3b54f"
      },
      "source": [
        "neg_pos_labels"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not confident',\n",
              " 'unsure',\n",
              " '',\n",
              " 'disordered',\n",
              " 'talkative',\n",
              " 'uninsterested',\n",
              " 'sad',\n",
              " 'unfriendly',\n",
              " 'confident',\n",
              " 'certany',\n",
              " 'handles pressure',\n",
              " 'organized',\n",
              " 'concise',\n",
              " 'interested',\n",
              " 'happy',\n",
              " 'friendly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH-eGFRwh5ax",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **1. Installations**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Wliixqgmo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3fb0c177-ab84-40d0-fc07-b672a6f8902c"
      },
      "source": [
        "!pip install nltk \n",
        "!pip install stanza\n",
        "!pip install spacy\n",
        "!pip3 install flair\n",
        "!pip install textblob\n",
        "\n",
        "!pip install emoji --upgrade\n",
        "\n",
        "!spacy download en_core_web_sm # sm md lg\n",
        "!python -m spacy download en"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (49.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: konoha[janome]<5.0.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.6.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from flair) (5.8)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (6.0.1)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.10)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.8)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.91)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.8.1rc1)\n",
            "Requirement already satisfied: overrides==3.0.0 in /usr/local/lib/python3.6/dist-packages (from konoha[janome]<5.0.0,>=4.0.0->flair) (3.0.0)\n",
            "Requirement already satisfied: janome<0.4.0,>=0.3.10; extra == \"janome\" or extra == \"all\" or extra == \"all_with_integrations\" in /usr/local/lib/python3.6/dist-packages (from konoha[janome]<5.0.0,>=4.0.0->flair) (0.3.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.0.1)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.7.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2020.6.20)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (1.14.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (1.17.48)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.15.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKQbIeZkiJ6t",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **2. Imports and downloads**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOcRIcOiiUu9",
        "colab_type": "text"
      },
      "source": [
        "## Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAIxYRUhiOWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from emoji import demojize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ombh-iahiXoc",
        "colab_type": "text"
      },
      "source": [
        "## NLTK\n",
        "* Words\n",
        "* Stopwords\n",
        "* WordNetLemmatizer\n",
        "* Vader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixlc1Gg2gV6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "9218a8b1-8a04-4636-beb4-6964d0fb4031"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# words\n",
        "NLTK_WORDS = set(words.words())\n",
        "\n",
        "# Stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Vader\n",
        "SIA = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpI3ZBfQjAoW",
        "colab_type": "text"
      },
      "source": [
        "## Stanza\n",
        "* ewt; tokenize, mwt, pos, lemma\n",
        "* default; tokenize, sentiment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBsjg0d5gde0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "b6fc6b75-758b-4940-c111-b34eefffaeaa"
      },
      "source": [
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stanza.download('en', package='default', processors='tokenize,sentiment', verbose=True)\n",
        "\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma,sentiment',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.7MB/s]                    \n",
            "2020-08-29 05:29:01 WARNING: Can not find mwt: ewt from official model list. Ignoring it.\n",
            "2020-08-29 05:29:01 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-08-29 05:29:01 INFO: File exists: /root/stanza_resources/en/tokenize/ewt.pt.\n",
            "2020-08-29 05:29:01 INFO: File exists: /root/stanza_resources/en/pos/ewt.pt.\n",
            "2020-08-29 05:29:01 INFO: File exists: /root/stanza_resources/en/lemma/ewt.pt.\n",
            "2020-08-29 05:29:02 INFO: File exists: /root/stanza_resources/en/pretrain/ewt.pt.\n",
            "2020-08-29 05:29:02 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 9.54MB/s]                    \n",
            "2020-08-29 05:29:02 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| sentiment | sstplus |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-08-29 05:29:02 INFO: File exists: /root/stanza_resources/en/tokenize/ewt.pt.\n",
            "2020-08-29 05:29:02 INFO: File exists: /root/stanza_resources/en/sentiment/sstplus.pt.\n",
            "2020-08-29 05:29:02 INFO: File exists: /root/stanza_resources/en/pretrain/ewt.pt.\n",
            "2020-08-29 05:29:02 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-08-29 05:29:02 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2020-08-29 05:29:02 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| sentiment | sstplus |\n",
            "=======================\n",
            "\n",
            "2020-08-29 05:29:02 INFO: Use device: cpu\n",
            "2020-08-29 05:29:02 INFO: Loading: tokenize\n",
            "2020-08-29 05:29:02 INFO: Loading: pos\n",
            "2020-08-29 05:29:03 INFO: Loading: lemma\n",
            "2020-08-29 05:29:03 INFO: Loading: sentiment\n",
            "2020-08-29 05:29:05 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzOmfrbjl_d",
        "colab_type": "text"
      },
      "source": [
        "## SpaCy\n",
        "* en_core_web sm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-w_t_p3ghAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "spNLP = spacy.load('en_core_web_sm')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work with stanza"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eKhFCbQkQn8",
        "colab_type": "text"
      },
      "source": [
        "## TextBlob\n",
        "* use a bag of words classifier, but the advantage is that it includes subjetivity analysis (factual/opinated)\n",
        "* it doesn't contain the heuristics that nltk has, it won't intensify or negate a sentence's sentiment\n",
        "\n",
        "* will return the subjectivity of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMXi5Ab0kSyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_FOj-1lxc1",
        "colab_type": "text"
      },
      "source": [
        "## **Flair**\n",
        "* classifier based on a character-leval LSTM. Takes a sequences of letters and words into account when predicting\n",
        "\n",
        "* one of its biggest advantages is that it can predict a sentiment for OOV words that it has never seen before too (such as typos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59AxHZvOl02c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2c92347-4619-4916-dc63-4a18dd330808"
      },
      "source": [
        "import flair\n",
        "flair_sent = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-29 05:29:08,093 loading file /root/.flair/models/sentiment-en-mix-distillbert.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65jn8kxAixOf",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **3. Functions**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P_X4V-omPLz",
        "colab_type": "text"
      },
      "source": [
        "## **i. Lemmatizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw0tfN636u4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVJN1YM610R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stanza\n",
        "def stanza_lemma(text):\n",
        "    doc = stNLP(text)\n",
        "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_g-MC8BmlPq",
        "colab_type": "text"
      },
      "source": [
        "## ii. Sentiment Analyzers\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze-Ry3MdnJd8",
        "colab_type": "text"
      },
      "source": [
        "### NLTK Vader\n",
        "* VADER, has different ratings depending on the form of the word and therefore the input should not be stemmed or lemmatized.\n",
        "\n",
        "* disadvantage of this approach is that Out of Vocab (OOV) words that the sentiment analysis tool has not seen before will not be classified as positive/negative (e.g. typos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaoEIpfAmn3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def siaVader_compound(text):\n",
        "    scores = SIA.polarity_scores(text)\n",
        "    \n",
        "    comp_score = scores['compound']\n",
        "    if comp_score >= 0.05:\n",
        "        str_comp = 'pos'\n",
        "    elif comp_score <= -0.05:\n",
        "        str_comp = 'neg'\n",
        "    else: # (compound score > -0.05) and (compound score < 0.05)\n",
        "        str_comp = 'neu'\n",
        "    return str_comp\n",
        "\n",
        "def siaVader_maxScore(text):\n",
        "    scores = SIA.polarity_scores(text)\n",
        "    \n",
        "    del scores['compound']\n",
        "    index = np.argmax(list(scores.values()))\n",
        "    vader_MaxScore = list(scores.values())[index]\n",
        "    vader_label = list(scores)[index]\n",
        "    \n",
        "    return vader_label\n",
        "\n",
        "###\n",
        "def siaVader_byWord(text):\n",
        "    c = 0\n",
        "    for n, y in enumerate(text):\n",
        "        x = SIA.polarity_scores(y)\n",
        "        if x['compound'] != 0.0:\n",
        "            c += 1\n",
        "            # print('{}. {} {}'.format(c, x, y))\n",
        "            return 'pos' if x > 0.05 else 'neg'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp4xW7dnnNsT",
        "colab_type": "text"
      },
      "source": [
        "### **TextBlob**\n",
        "* use a bag of words classifier, but the advantage is that it includes subjetivity analysis (factual/opinated)\n",
        "* it doesn't contain the heuristics that nltk has, it won't intensify or negate a sentence's sentiment\n",
        "\n",
        "* will return the subjectivity of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfcolyImpQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_blob_subject(text):\n",
        "    return TextBlob(text).sentiment"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvP-jgenSqD",
        "colab_type": "text"
      },
      "source": [
        "### **Flair LSTM**\n",
        "* classifier based on a character-leval LSTM. Takes a sequences of letters and words into account when predicting\n",
        "\n",
        "* one of its biggest advantages is that it can predict a sentiment for OOV words that it has never seen before too (such as typos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR0Ic6OkmtKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flair_lstm(text):\n",
        "    sentence = flair.data.Sentence(text)\n",
        "    flair_sent.predict(sentences=sentence)\n",
        "    total_sent = sentence.labels\n",
        "    return total_sent "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlxoeIP9o6ZN",
        "colab_type": "text"
      },
      "source": [
        "### **Stanza**\n",
        "* stanza pipeline by using a CNN classifier.\n",
        "* training this model on 2 class data using higher dimension word vectors achieves the 87 score reported in the original CNN classifier paper. On a three class projection of the SST test data, the model trained on multiple datasets gets 70.0%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFcUZ67ZpC6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stanza_funct(text):\n",
        "    data = stNLP(text)\n",
        "    for sentence in data.sentences:\n",
        "        return sentence.sentiment"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo-atzwUmZdy",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## **iii. Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FbDdGQcwqPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path=None):\n",
        "    print('load the dataset...\\n')\n",
        "    !mkdir -p data\n",
        "    !wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "    !unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmCEUmBPmbia",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **4. Preprocess dataset**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I71x7SvXwmH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_dataset(PATH_FILE, index_col=None):\n",
        "    print('preprocess the dataset...\\n')\n",
        "\n",
        "    # load_data\n",
        "    load_data()\n",
        "    print('Database loaded\\n')\n",
        "\n",
        "    # cleaning data\n",
        "    unclean_df = pd.read_csv(PATH_FILE,\n",
        "                     names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                     encoding='latin-1') # if utf-8: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 232719-232720: invalid continuation byte\n",
        "\n",
        "    # replace polarity\n",
        "    unclean_df.polarity = unclean_df.polarity.replace({0: 0, 4: 1}) \n",
        "    \n",
        "    # dropping unneeded columns\n",
        "    unclean_df = unclean_df.drop(columns=['id', 'date', 'query', 'user']) \n",
        "\n",
        "    # lower case\n",
        "    unclean_df['text'] = unclean_df['text'].str.lower()\n",
        "\n",
        "    # removing urls\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'https://www\\.|http:\\.|https://|www\\.', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil|cl)[\\S]*\\s?', '', x))\n",
        "\n",
        "    # remove special character and numbers\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-zÁ-Úá-ú \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?%', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "    # remove repetitions (goood ==> good ==> god?; whaaat ==> what)\n",
        "    pattern = re.compile(r'(.)\\1{2,}', re.DOTALL)\n",
        "    unclean_df['text'] = unclean_df['text'].str.replace(pattern, r'\\1')\n",
        "\n",
        "    # removing empty values\n",
        "    nan_value = float('NaN')\n",
        "    unclean_df.replace('', nan_value, inplace=True)\n",
        "    unclean_df.dropna(inplace=True) # add subset\n",
        "\n",
        "    # removing stopwords\n",
        "    #df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i not in (STOPWORDS)]))\n",
        "\n",
        "    # filtering and removing non-english words or misspelling\n",
        "    #df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i.lower() in NLTK_WORDS or not i.isalpha()]))\n",
        "\n",
        "    # rewritting the created file without NaN values\n",
        "    unclean_df.to_csv('data/sentiment140-subset.csv', \n",
        "              quotechar='\"', # check later!\n",
        "              encoding='utf-8',\n",
        "              index=False)\n",
        "\n",
        "    # clean csv\n",
        "    df = pd.read_csv('data/sentiment140-subset.csv', encoding='utf-8', warn_bad_lines=True).dropna()\n",
        "\n",
        "    # checking if there's any NaN values\n",
        "    isnull = [i for i in (df['text'].isnull()) if i == True]\n",
        "    if isnull != []:\n",
        "        sys.exit(0) # add response object here\n",
        "\n",
        "    return df"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CinwlW5wxvxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "d3d4b960-5e55-45ee-ddf6-207749dabafb"
      },
      "source": [
        "df = preprocess_dataset(PATH_FILE='data/training.1600000.processed.noemoticon.csv')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocess the dataset...\n",
            "\n",
            "load the dataset...\n",
            "\n",
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "Database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iREYVprZwLRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from emoji import demojize, emojize\n",
        "# removing emojis\n",
        "#df['text'] = df['text'].apply(lambda x: demojize(string=x))\n",
        "#df_emojis = df['text'].apply(lambda x: re.findall(r':[a-z_]+:', string=demojize(x)))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTolK81BQnvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for n, i in enumerate(df.emojis):\n",
        "#    if i != []:\n",
        "#        print(n, emojize(str(i)))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ap_AgDrDhUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_csv = df.to_csv('checking_csv.csv', quotechar='\"', encoding='utf-8')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5U88c3AyhbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "487ad293-8060-402a-b6f9-c2ec77553851"
      },
      "source": [
        "'''\n",
        "emo_dict = {}\n",
        "for i, j in df_emojis.iteritems():\n",
        "    for k in j:\n",
        "        if k in emo_dict:\n",
        "            emo_dict[k] += 1\n",
        "        else:\n",
        "            emo_dict[k] = 1\n",
        "\n",
        "df_hashtags = df['text'].apply(lambda x: re.findall(r'#/S+', string=x))\n",
        "hashtags = {}\n",
        "for i, j in df_hashtags.iteritems(): \n",
        "    for k in j:\n",
        "        if k in hashtags:\n",
        "            hashtags[k] += 1\n",
        "        else:\n",
        "            hashtags[k] = 1\n",
        "            \n",
        "for i, c in sorted(emo_dict.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(emojize(i) + i + str(c))\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nemo_dict = {}\\nfor i, j in df_emojis.iteritems():\\n    for k in j:\\n        if k in emo_dict:\\n            emo_dict[k] += 1\\n        else:\\n            emo_dict[k] = 1\\n\\ndf_hashtags = df['text'].apply(lambda x: re.findall(r'#/S+', string=x))\\nhashtags = {}\\nfor i, j in df_hashtags.iteritems(): \\n    for k in j:\\n        if k in hashtags:\\n            hashtags[k] += 1\\n        else:\\n            hashtags[k] = 1\\n            \\nfor i, c in sorted(emo_dict.items(), key=lambda x: x[1], reverse=True):\\n    print(emojize(i) + i + str(c))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Me-JFlxrhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing\n",
        "df_test = pd.read_csv('emo_test.csv', sep='\\t')\n",
        "df_test['SIA'] = df_test['word'].apply(lambda x: siaVader_compound(x))\n",
        "df_test['comp_val'] = df_test['word'].apply(lambda x: (SIA.polarity_scores(x))['compound'])\n",
        "df_test['VADER_pos_neg'] = df_test['word'].apply(\n",
        "    lambda x: SIA.polarity_scores(x)['pos'] if SIA.polarity_scores(x)['pos'] > SIA.polarity_scores(x)['neg'] else SIA.polarity_scores(x)['neg']\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "friq1FCSy8GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['flair'] = df_test['word'].apply(lambda x: flair_lstm(x))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8lMisAgMR0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['comp_label'] = df['text'].apply(lambda x: siaVader_compound(text=x))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13FO26_dgplF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['maxScore_label'] = df['text'].apply(lambda x: siaVader_maxScore(text=x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXT4vF2ry7w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['comp_score'] = df['text'].apply(lambda x: SIA.polarity_scores(x)['compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnVDjtKzYJv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['SIA'] = df['text'].apply(lambda x: SIA.polarity_scores(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ychJGKqt1rTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['flair'] = df['text'].apply(lambda x: flair_lstm(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed4C-h7I4oQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag0SHAvrFEjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('check_csv_2.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhA00Ek3p0wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail(60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}