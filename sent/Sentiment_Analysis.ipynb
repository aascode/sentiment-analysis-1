{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCWXCBVHzweU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "504d6a77-fe9e-4b15-967c-652d2a45fc49",
        "tags": []
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "# !pip install tensorflow_text # I could use BERT (?)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "^C\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKssXJ-F5FGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "37bf1692-29fb-4b0f-d646-e6829604a3f0"
      },
      "source": [
        "import logging\n",
        "from psutil import virtual_memory\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "\n",
        "tf_response = {\n",
        "    'error': None,\n",
        "    'TF version': '',\n",
        "    'COLAB': None,\n",
        "    'GPU': False,\n",
        "    'ram_gb': ''\n",
        "}\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    tf_response['COLAB'] = COLAB\n",
        "except OSError as error:\n",
        "    COLAB = False\n",
        "    response['error'] = logging.debug('You are not using your specify version of TensorFlow')\n",
        "    tf_response['COLAB'] = COLAB\n",
        "finally:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        GPU = True\n",
        "        tf_response['GPU'] = GPU\n",
        "    tf_response['TF_version'] = tf.__version__\n",
        "    \n",
        "    if tf_response['COLAB'] == True:\n",
        "        if gpu_info.find('failed') >= 0:\n",
        "            print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator')\n",
        "            print('Re-execute this cell.')\n",
        "        else:\n",
        "            print(gpu_info)\n",
        "        \n",
        "        if ram_gb < 20:\n",
        "            print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type menu\"')\n",
        "            print('Select high-RAM in the runtime shape dropdown')\n",
        "            print('Re-execute this cell')\n",
        "            tf_response['ram_gb'] = 'low-RAM runtime'\n",
        "        else:\n",
        "            tf_response['ram_gb'] = 'high-RAM runtime'\n",
        "        print('\\nRuntime {:.2f} GB of available RAM\\n'.format(ram_gb))\n",
        "    print(tf_response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 22 17:12:23 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "Runtime 27.39 GB of available RAM\n",
            "\n",
            "{'error': None, 'TF version': '', 'COLAB': True, 'GPU': True, 'ram_gb': 'high-RAM runtime', 'TF_version': '2.3.0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW1C6W9DQws8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBIi3kzrV3ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "03167214-23fe-4ac2-898a-abbbe44ed1f7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sweo4LneWAfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "e4858f54-6069-4f9b-ab58-bc4a4e4ca70b"
      },
      "source": [
        "# Stanza NLP\n",
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (49.2.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.6MB/s]                    \n",
            "2020-08-22 17:12:42 WARNING: Can not find mwt: ewt from official model list. Ignoring it.\n",
            "2020-08-22 17:12:42 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-08-22 17:12:42 INFO: File exists: /root/stanza_resources/en/tokenize/ewt.pt.\n",
            "2020-08-22 17:12:43 INFO: File exists: /root/stanza_resources/en/pos/ewt.pt.\n",
            "2020-08-22 17:12:43 INFO: File exists: /root/stanza_resources/en/lemma/ewt.pt.\n",
            "2020-08-22 17:12:48 INFO: File exists: /root/stanza_resources/en/pretrain/ewt.pt.\n",
            "2020-08-22 17:12:48 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-08-22 17:12:48 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2020-08-22 17:12:48 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-08-22 17:12:48 INFO: Use device: gpu\n",
            "2020-08-22 17:12:48 INFO: Loading: tokenize\n",
            "2020-08-22 17:12:57 INFO: Loading: pos\n",
            "2020-08-22 17:12:58 INFO: Loading: lemma\n",
            "2020-08-22 17:12:58 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zH74LpiW1aM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "617e1bca-3ead-45fc-c979-a53487294f5e"
      },
      "source": [
        "doc = stNLP('Barack Obama was born in Hawai.')\n",
        "\n",
        "print('\\n')\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: was \tlemma: be\n",
            "word: born \tlemma: bear\n",
            "word: in \tlemma: in\n",
            "word: Hawai \tlemma: Hawai\n",
            "word: . \tlemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aacz6_0m5xvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "779567d1-88d0-4ba9-973b-bcffe5ee07d1"
      },
      "source": [
        "!pip install spacy\n",
        "!spacy download en_core_web_lg # sm md lg\n",
        "import spacy\n",
        "spNLP = spacy.load('en_core_web_lg')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work with stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (49.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW0G_4k17U18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_lenght = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "training_portion = .8\n",
        "\n",
        "PATH_FILE = 'data/training.1600000.processed.noemoticon.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G21PzPJFYEAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path=None):\n",
        "    !mkdir -p data\n",
        "    !wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "    !unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip\n",
        "    \n",
        "    # return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npocB7yHDfHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "021afdbe-e071-4fe0-f016-4889bb2dcb3c"
      },
      "source": [
        "# lemmatizion\n",
        "# stanza\n",
        "def stanza_lemma(text):\n",
        "    doc = stNLP(text)\n",
        "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
        "\n",
        "# nltk\n",
        "!pip install nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wO_zeUcFHUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_dataset():\n",
        "    # load_data\n",
        "    load_data()\n",
        "    print('database loaded\\n')\n",
        "\n",
        "    # cleaning data\n",
        "    unclean_df = pd.read_csv(PATH_FILE,\n",
        "                     names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                     encoding='latin-1') # if utf-8: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 232719-232720: invalid continuation byte\n",
        "\n",
        "    unclean_df.polarity = unclean_df.polarity.replace({0: 0, 4: 1}) # replace polarity\n",
        "    unclean_df = unclean_df.drop(columns=['id', 'date', 'query', 'user']) # dropping unneeded columns\n",
        "\n",
        "    # sample\n",
        "    #df_sample = unclean_df.sample(n=500000)\n",
        "    #df_sample.polarity.value_counts()\n",
        "\n",
        "    # lower case\n",
        "    unclean_df['text'] = unclean_df['text'].str.lower()\n",
        "\n",
        "    # remove character and numbers\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'https://www\\.|http:\\.|https://|www\\.', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil|cl)[\\S]*\\s?', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-zÁ-Úá-ú \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?%', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "\n",
        "    # rewritting the created file without NaN values\n",
        "    unclean_df.to_csv('data/sentiment140-subset.csv', \n",
        "              quotechar='\"', # check later!\n",
        "              encoding='latin-1',\n",
        "              index=False)\n",
        "\n",
        "    # clean csv\n",
        "    df = pd.read_csv('data/sentiment140-subset.csv', encoding='latin-1').dropna()\n",
        "\n",
        "    # checking if there's any NaN values\n",
        "    isnull = [i for i in (df['text'].isnull()) if i == True]\n",
        "    if isnull != []:\n",
        "        sys.exit(0) # add response object here\n",
        "\n",
        "    \n",
        "    # STOPWORDS\n",
        "    # Getting in a list all the stopwords of the dataframe \n",
        "    '''\n",
        "    spacy_stop_words = list(dict.fromkeys([str(i) for i in spNLP(' '.join(j for j in df['text'])) if i.is_stop == True]))\n",
        "\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.extend(spacy_stop_words)\n",
        "    stop_words = set(stop_words)\n",
        "    \n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i not in stop_words]))\n",
        "    \n",
        "    # Lemmatization Stanza vs NLTK\n",
        "    df['text'] = df['text'].apply(lambda x: stanza_lemma(x))\n",
        "    df['text'] = df['text'].apply(lambda x: nltk_lemma(x))\n",
        "\n",
        "    # check new stopwords here!\n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join(\n",
        "        [i for i in x.split() if i not in stop_words]\n",
        "    ))\n",
        "    '''\n",
        "    # return df\n",
        "    x, y = df.text, df.polarity\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtGPDu4aFxQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ba848fb1-226c-437c-cd82-7d5f0ee768c4"
      },
      "source": [
        "# test peprocess_dataset\n",
        "x, y = preprocess_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJqblmY3KNJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "ec0fbed9-130f-4106-b06d-977c1ef6163f"
      },
      "source": [
        "# test peprocess_dataset\n",
        "print(y.value_counts())\n",
        "y.value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    799982\n",
            "1    799969\n",
            "Name: polarity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD1CAYAAAClSgmzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUdElEQVR4nO3df6zd9X3f8eeruLQ0XYJD7ixqmxkpXiMHKQlY4CrTtIXVGDrV/JFEoGlYyIonhWzNMmlx9o81GBKRprEiJZas4sWeuhCXNcLKnLiWk6iqJhNfEgYBynxLQmyLH7e2A2uzJCV974/z8XK4OZ9zjwmc6+DnQzo63+/78/l8P58jXZ2Xz/f7PT6pKiRJGuWXlnoBkqTzlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqSuZUu9gNfbO97xjlqzZs1SL0OSfqE88sgjf1lVMwvrb7qQWLNmDbOzs0u9DEn6hZLk2VF1TzdJkroMCUlSlyEhSeoyJCRJXYaEJKlropBI8q+TPJHk20k+n+RXk1yZ5OEkc0m+kOTi1vdX2v5ca18zdJxPtfrTSW4Yqm9qtbkk24fqI+eQJE3HoiGRZCXwr4D1VXUVcBFwC/Bp4N6qeidwBtjahmwFzrT6va0fSda1ce8GNgGfTXJRkouAzwA3AuuAW1tfxswhSZqCSU83LQMuSbIM+DXgOeADwIOtfQ9wc9ve3PZp7dcnSas/UFU/qqrvAHPAte0xV1XPVNWPgQeAzW1Mbw5J0hQs+mW6qjqZ5D8C3wP+L/AnwCPA96vqldbtBLCyba8EjrexryR5Cbis1Y8MHXp4zPEF9evamN4cr5JkG7AN4IorrljsJZ0X1mz/H0u9hDeN797zO0u9hDcV/zZfX7/of5+TnG5azuBTwJXAbwBvYXC66LxRVbuqan1VrZ+Z+ZlvlUuSXqNJTjf9E+A7VTVfVX8D/DHwfuDSdvoJYBVwsm2fBFYDtPa3AaeG6wvG9OqnxswhSZqCSULie8CGJL/WrhNcDzwJfA34YOuzBXiobe9v+7T2r9bgh7T3A7e0u5+uBNYC3wCOAmvbnUwXM7i4vb+N6c0hSZqCRUOiqh5mcPH4m8Djbcwu4JPAJ5LMMbh+cH8bcj9wWat/AtjejvMEsI9BwHwFuKOqftKuOXwMOAg8BexrfRkzhyRpCib6X2CragewY0H5GQZ3Ji3s+0PgQ53j3A3cPaJ+ADgwoj5yDknSdPiNa0lSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSuhYNiSS/meTRocfLST6e5O1JDiU51p6Xt/5Jcl+SuSSPJbl66FhbWv9jSbYM1a9J8ngbc1/7mVR6c0iSpmOSny99uqreW1XvBa4BfgB8kcHPkh6uqrXA4bYPcCOD369eC2wDdsLgDZ/Br9tdx+DX5nYMvenvBD4yNG5Tq/fmkCRNwbmebroe+IuqehbYDOxp9T3AzW17M7C3Bo4Alya5HLgBOFRVp6vqDHAI2NTa3lpVR6qqgL0LjjVqDknSFJxrSNwCfL5tr6iq59r288CKtr0SOD405kSrjaufGFEfN4ckaQomDokkFwO/C/zRwrb2CaBex3X9jHFzJNmWZDbJ7Pz8/Bu5DEm6oJzLJ4kbgW9W1Qtt/4V2qoj2/GKrnwRWD41b1Wrj6qtG1MfN8SpVtauq1lfV+pmZmXN4SZKkcc4lJG7lp6eaAPYDZ+9Q2gI8NFS/rd3ltAF4qZ0yOghsTLK8XbDeCBxsbS8n2dDuarptwbFGzSFJmoJlk3RK8hbgt4F/MVS+B9iXZCvwLPDhVj8A3ATMMbgT6naAqjqd5C7gaOt3Z1WdbtsfBT4HXAJ8uT3GzSFJmoKJQqKq/hq4bEHtFIO7nRb2LeCOznF2A7tH1GeBq0bUR84hSZoOv3EtSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6pooJJJcmuTBJH+e5Kkkv5Xk7UkOJTnWnpe3vklyX5K5JI8luXroOFta/2NJtgzVr0nyeBtzX/uta3pzSJKmY9JPEr8PfKWq3gW8B3gK2A4crqq1wOG2D3AjsLY9tgE7YfCGD+wArgOuBXYMvenvBD4yNG5Tq/fmkCRNwaIhkeRtwD8E7geoqh9X1feBzcCe1m0PcHPb3gzsrYEjwKVJLgduAA5V1emqOgMcAja1trdW1ZH2+9h7Fxxr1BySpCmY5JPElcA88F+SfCvJHyR5C7Ciqp5rfZ4HVrTtlcDxofEnWm1c/cSIOmPmkCRNwSQhsQy4GthZVe8D/poFp33aJ4B6/Zc32RxJtiWZTTI7Pz//Ri5Dki4ok4TECeBEVT3c9h9kEBovtFNFtOcXW/tJYPXQ+FWtNq6+akSdMXO8SlXtqqr1VbV+ZmZmgpckSZrEoiFRVc8Dx5P8ZitdDzwJ7AfO3qG0BXiobe8Hbmt3OW0AXmqnjA4CG5MsbxesNwIHW9vLSTa0u5puW3CsUXNIkqZg2YT9/iXwh0kuBp4BbmcQMPuSbAWeBT7c+h4AbgLmgB+0vlTV6SR3AUdbvzur6nTb/ijwOeAS4MvtAXBPZw5J0hRMFBJV9SiwfkTT9SP6FnBH5zi7gd0j6rPAVSPqp0bNIUmaDr9xLUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeqaKCSSfDfJ40keTTLbam9PcijJsfa8vNWT5L4kc0keS3L10HG2tP7HkmwZql/Tjj/XxmbcHJKk6TiXTxL/uKreW1Vnf8Z0O3C4qtYCh9s+wI3A2vbYBuyEwRs+sAO4DrgW2DH0pr8T+MjQuE2LzCFJmoKf53TTZmBP294D3DxU31sDR4BLk1wO3AAcqqrTVXUGOARsam1vraoj7fex9y441qg5JElTMGlIFPAnSR5Jsq3VVlTVc237eWBF214JHB8ae6LVxtVPjKiPm0OSNAXLJuz3D6rqZJK/CxxK8ufDjVVVSer1X95kc7Tg2gZwxRVXvJHLkKQLykSfJKrqZHt+Efgig2sKL7RTRbTnF1v3k8DqoeGrWm1cfdWIOmPmWLi+XVW1vqrWz8zMTPKSJEkTWDQkkrwlyd85uw1sBL4N7AfO3qG0BXiobe8Hbmt3OW0AXmqnjA4CG5MsbxesNwIHW9vLSTa0u5puW3CsUXNIkqZgktNNK4AvtrtSlwH/raq+kuQosC/JVuBZ4MOt/wHgJmAO+AFwO0BVnU5yF3C09buzqk637Y8CnwMuAb7cHgD3dOaQJE3BoiFRVc8A7xlRPwVcP6JewB2dY+0Gdo+ozwJXTTqHJGk6/Ma1JKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqWvikEhyUZJvJflS278yycNJ5pJ8IcnFrf4rbX+uta8ZOsanWv3pJDcM1Te12lyS7UP1kXNIkqbjXD5J/B7w1ND+p4F7q+qdwBlga6tvBc60+r2tH0nWAbcA7wY2AZ9twXMR8BngRmAdcGvrO24OSdIUTBQSSVYBvwP8QdsP8AHgwdZlD3Bz297c9mnt17f+m4EHqupHVfUdYA64tj3mquqZqvox8ACweZE5JElTMOknif8M/Fvgb9v+ZcD3q+qVtn8CWNm2VwLHAVr7S63//68vGNOrj5tDkjQFi4ZEkn8KvFhVj0xhPa9Jkm1JZpPMzs/PL/VyJOlNY5JPEu8HfjfJdxmcCvoA8PvApUmWtT6rgJNt+ySwGqC1vw04NVxfMKZXPzVmjlepql1Vtb6q1s/MzEzwkiRJk1g0JKrqU1W1qqrWMLjw/NWq+mfA14APtm5bgIfa9v62T2v/alVVq9/S7n66ElgLfAM4CqxtdzJd3ObY38b05pAkTcHP8z2JTwKfSDLH4PrB/a1+P3BZq38C2A5QVU8A+4Anga8Ad1TVT9o1h48BBxncPbWv9R03hyRpCpYt3uWnqurrwNfb9jMM7kxa2OeHwIc64+8G7h5RPwAcGFEfOYckaTr8xrUkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpa9GQSPKrSb6R5H8leSLJv2/1K5M8nGQuyRfa71PTfsP6C63+cJI1Q8f6VKs/neSGofqmVptLsn2oPnIOSdJ0TPJJ4kfAB6rqPcB7gU1JNgCfBu6tqncCZ4Ctrf9W4Eyr39v6kWQdcAvwbmAT8NkkFyW5CPgMcCOwDri19WXMHJKkKVg0JGrgr9ruL7dHAR8AHmz1PcDNbXtz26e1X58krf5AVf2oqr4DzDH4/eprgbmqeqaqfgw8AGxuY3pzSJKmYKJrEu1f/I8CLwKHgL8Avl9Vr7QuJ4CVbXslcBygtb8EXDZcXzCmV79szBySpCmYKCSq6idV9V5gFYN/+b/rDV3VOUqyLclsktn5+fmlXo4kvWmc091NVfV94GvAbwGXJlnWmlYBJ9v2SWA1QGt/G3BquL5gTK9+aswcC9e1q6rWV9X6mZmZc3lJkqQxJrm7aSbJpW37EuC3gacYhMUHW7ctwENte3/bp7V/taqq1W9pdz9dCawFvgEcBda2O5kuZnBxe38b05tDkjQFyxbvwuXAnnYX0i8B+6rqS0meBB5I8h+AbwH3t/73A/81yRxwmsGbPlX1RJJ9wJPAK8AdVfUTgCQfAw4CFwG7q+qJdqxPduaQJE3BoiFRVY8B7xtRf4bB9YmF9R8CH+oc627g7hH1A8CBSeeQJE2H37iWJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdU3yG9erk3wtyZNJnkjye63+9iSHkhxrz8tbPUnuSzKX5LEkVw8da0vrfyzJlqH6NUkeb2PuS5Jxc0iSpmOSTxKvAP+mqtYBG4A7kqwDtgOHq2otcLjtA9wIrG2PbcBOGLzhAzuA6xj8JOmOoTf9ncBHhsZtavXeHJKkKVg0JKrquar6Ztv+P8BTwEpgM7CnddsD3Ny2NwN7a+AIcGmSy4EbgENVdbqqzgCHgE2t7a1VdaSqCti74Fij5pAkTcE5XZNIsgZ4H/AwsKKqnmtNzwMr2vZK4PjQsBOtNq5+YkSdMXNIkqZg4pBI8uvAfwc+XlUvD7e1TwD1Oq/tVcbNkWRbktkks/Pz82/kMiTpgjJRSCT5ZQYB8YdV9cet/EI7VUR7frHVTwKrh4avarVx9VUj6uPmeJWq2lVV66tq/czMzCQvSZI0gUnubgpwP/BUVf2noab9wNk7lLYADw3Vb2t3OW0AXmqnjA4CG5MsbxesNwIHW9vLSTa0uW5bcKxRc0iSpmDZBH3eD/xz4PEkj7bavwPuAfYl2Qo8C3y4tR0AbgLmgB8AtwNU1ekkdwFHW787q+p02/4o8DngEuDL7cGYOSRJU7BoSFTVnwHpNF8/on8Bd3SOtRvYPaI+C1w1on5q1BySpOnwG9eSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkrkl+43p3kheTfHuo9vYkh5Ica8/LWz1J7ksyl+SxJFcPjdnS+h9LsmWofk2Sx9uY+9rvXHfnkCRNzySfJD4HbFpQ2w4crqq1wOG2D3AjsLY9tgE7YfCGD+wArgOuBXYMvenvBD4yNG7TInNIkqZk0ZCoqj8FTi8obwb2tO09wM1D9b01cAS4NMnlwA3Aoao6XVVngEPAptb21qo60n4be++CY42aQ5I0Ja/1msSKqnqubT8PrGjbK4HjQ/1OtNq4+okR9XFzSJKm5Oe+cN0+AdTrsJbXPEeSbUlmk8zOz8+/kUuRpAvKaw2JF9qpItrzi61+Elg91G9Vq42rrxpRHzfHz6iqXVW1vqrWz8zMvMaXJEla6LWGxH7g7B1KW4CHhuq3tbucNgAvtVNGB4GNSZa3C9YbgYOt7eUkG9pdTbctONaoOSRJU7JssQ5JPg/8I+AdSU4wuEvpHmBfkq3As8CHW/cDwE3AHPAD4HaAqjqd5C7gaOt3Z1WdvRj+UQZ3UF0CfLk9GDOHJGlKFg2Jqrq103T9iL4F3NE5zm5g94j6LHDViPqpUXNIkqbHb1xLkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSus77kEiyKcnTSeaSbF/q9UjSheS8DokkFwGfAW4E1gG3Jlm3tKuSpAvHeR0SwLXAXFU9U1U/Bh4ANi/xmiTpgrFsqRewiJXA8aH9E8B1Czsl2QZsa7t/leTpKaztQvEO4C+XehHj5NNLvQItkfP+bxN+of4+/96o4vkeEhOpql3ArqVex5tRktmqWr/U65AW8m9zOs73000ngdVD+6taTZI0Bed7SBwF1ia5MsnFwC3A/iVekyRdMM7r001V9UqSjwEHgYuA3VX1xBIv60LjaTydr/zbnIJU1VKvQZJ0njrfTzdJkpaQISFJ6jIkJEld5/WFa01Xkncx+Eb7ylY6CeyvqqeWblWSlpKfJARAkk8y+G9PAnyjPQJ83v9YUeezJLcv9RrezLy7SQAk+d/Au6vqbxbULwaeqKq1S7Myabwk36uqK5Z6HW9Wnm7SWX8L/Abw7IL65a1NWjJJHus1ASumuZYLjSGhsz4OHE5yjJ/+p4pXAO8EPrZkq5IGVgA3AGcW1AP8z+kv58JhSAiAqvpKkr/P4L9nH75wfbSqfrJ0K5MA+BLw61X16MKGJF+f/nIuHF6TkCR1eXeTJKnLkJAkdRkSkqQuQ0KS1GVISJK6/h9dXuTZW07rJgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQHKelS39Zan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6419afcd-061c-43ed-ef4e-a1ca45632b12"
      },
      "source": [
        "# test peprocess_dataset\n",
        "print(Counter(x).most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(' ', 2301), (' thanks ', 1572), (' get  followers a day using once you add everyone you are on the train or pay vip ', 1324), (' thank you ', 998), (' i am lost please help me find a good home ', 507), (' me too ', 453), ('good morning ', 444), (' good morning ', 355), (' i know ', 342), ('  ', 310)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci01PpVNYdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(test_size=0.2, validation_size=0.2):\n",
        "    # split dataset (as string into panda.core.series.Serie object)\n",
        "    (x, y) = preprocess_dataset()\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
        "\n",
        "    # testing\n",
        "    print(len(x_train), len(y_train))     \n",
        "    #print(len(x_validation, y_validation)\n",
        "    print(len(x_test), len(y_test))\n",
        "\n",
        "    # return (x_train, y_train), (x_validation, y_validation), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKY4MEpMiQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e7722060-b462-4645-92f8-88b77b164224"
      },
      "source": [
        "# test prepare_dataset\n",
        "prepare_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "database loaded\n",
            "\n",
            "1279960 1279960\n",
            "319991 319991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iasm0J2eYsvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape, learning_rate=0.0001, opt='adam', loss='categorical_crossentropy'):\n",
        "    model = tf.keras.layers.Sequential()\n",
        "\n",
        "    # layers\n",
        "    model.add(Embedding(vocab_size, embedding_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(embedding_dim)))\n",
        "\n",
        "    # softmax output layer\n",
        "    model.add(tf.keras.layers.Dense(units=10, \n",
        "                                    activation='softmax'))\n",
        "\n",
        "    # optimizer & loss\n",
        "    opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss='sparse_categorical_crossentropy'\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss,\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OaqUi0_ZwH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, x_train, y_train, x_validation, y_validation, epochs, batch_size=32, patience=5, verbose=1, monitor='accuracy'):\n",
        "    # callback\n",
        "    early_callback = tf.keras.callbacks.EarlyStopping(monitor=monitor, # also try 'val_loss'\n",
        "                                                      verbose=1, mode='auto', restore_best_weights=True,\n",
        "                                                      min_delta=1e-3, patience=patience)\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=verbose\n",
        "                        validation_data=(x_validation, y_validation), # x_test, y_test\n",
        "                        callbacks=[early_callback])\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsuytCdNDyDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history_(history):\n",
        "    fitModel_dict = history.history\n",
        "    acc = fitModel_dict['accuracy']\n",
        "    val_acc = fitModel_dict['val_accuracy']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.ylim((0.5, 1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_history(history, string):\n",
        "    fitModel_dict = history.history\n",
        "    plt.plot(fitModel_dict[string])\n",
        "    plt.plot/fitModel_dict['val_' + string]\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_' + string])\n",
        "    plt.show()\n",
        "\n",
        "# test\n",
        "plot_history(history, 'accuracy')\n",
        "plot_history(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvUnHHkbykJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    x_train, y_train, x_validation, y_validation, x_test, y_test = prepare_dataset(data_path) # define data path for google colab and visual studio with statements\n",
        "\n",
        "    # input_shape\n",
        "    # model = build_model()\n",
        "\n",
        "    # history = train(model)\n",
        "    # plot_history(history)\n",
        "\n",
        "    # test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    # print('\\nTest:\\nLoss: {}\\nAccuracy: {}').format(loss, accuracy * 100)\n",
        "\n",
        "    # model.save(model.h5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBushApRdFKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}