{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpAtmXkvvWzi",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **Sentiment Analysis**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWLdiIVBryE7",
        "colab_type": "text"
      },
      "source": [
        "# **Categories**\n",
        "\n",
        "## **1. as text analysis**\n",
        "- confident --> else: word not confident\n",
        "- happy/joyful words --> else:  ...\n",
        "- enthusiasm --> \"I enjoy\", \"I love\", \"I take pride in\"\n",
        "\n",
        "- slang words\n",
        "\n",
        "- filler words? : what words\n",
        "\n",
        "- focus --> concise, answer lenght\n",
        "\n",
        "## **2. as speech recognition (tone)**\n",
        "- engaged --> tone, bored, desinerested, certainm ...\n",
        "\n",
        "- long pause? : output\n",
        "\n",
        "## **3. once 1 and 2 is finished**\n",
        "- quantifying  first impressions\n",
        "- answer pace\n",
        "- higher level groupings --> confidence + certain = proffesional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_IfHaPNDmH",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **1. Installation**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzL6TXxjOpdG",
        "colab_type": "text"
      },
      "source": [
        "## i. Generating a reponse\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSlAEpfQzo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import logging\n",
        "from psutil import virtual_memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HY2EDdOQxZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "ram_gb = virtual_memory().total / 1e9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xrnkV_nQ2dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_response = {\n",
        "    'error': None,\n",
        "    'TF version': '',\n",
        "    'COLAB': None,\n",
        "    'GPU': False,\n",
        "    'ram_gb': ''\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_99wr64YOQpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    # drive\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "    # updating tensorflow version\n",
        "    %tensorflow_version 2.x\n",
        "\n",
        "    # tensorflow-gpu\n",
        "    !pip install tensorflow-gpu # !pip install tensorflow_text # I could use BERT\n",
        "    \n",
        "    # NLP (nltk, stanza, spacy)\n",
        "    !pip install nltk \n",
        "    !pip install stanza\n",
        "    !pip install spacy\n",
        "    !spacy download en_core_web_sm # sm md lg\n",
        "    !python -m spacy download en\n",
        "except OSError as error:\n",
        "    # debugging error\n",
        "    response['error'] = logging.debug('You are not using your specify version of TensorFlow')\n",
        "    IN_COLAB = False\n",
        "\n",
        "    # install requirements\n",
        "    !pip install -r '../requirements.txt'\n",
        "finally:\n",
        "    tf_response['COLAB'] = IN_COLAB\n",
        "    \n",
        "    # Importing tensroflow core\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # GPU and RAM response\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "            GPU = True\n",
        "            tf_response['GPU'] = GPU\n",
        "        tf_response['TF_version'] = tf.__version__\n",
        "        \n",
        "        if tf_response['COLAB'] == True:\n",
        "            if gpu_info.find('failed') >= 0:\n",
        "                print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator')\n",
        "                print('Re-execute this cell.')\n",
        "            else:\n",
        "                print(gpu_info)\n",
        "            \n",
        "            if ram_gb < 20:\n",
        "                print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type menu\"')\n",
        "                print('Select high-RAM in the runtime shape dropdown')\n",
        "                print('Re-execute this cell')\n",
        "                tf_response['ram_gb'] = 'low-RAM runtime'\n",
        "            else:\n",
        "                tf_response['ram_gb'] = 'high-RAM runtime'\n",
        "            print('\\nRuntime {:.2f} GB of available RAM\\n'.format(ram_gb))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7s0vMHrRS59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF6hVyvJSEN9",
        "colab_type": "text"
      },
      "source": [
        "## ii. Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW1C6W9DQws8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data analysis\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "# Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBIi3kzrV3ak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sweo4LneWAfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stanza NLP\n",
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zH74LpiW1aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing stanza\n",
        "doc = stNLP('Barack Obama was born in Hawai.')\n",
        "print('\\n')\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aacz6_0m5xvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spacy NLP\n",
        "import spacy\n",
        "spNLP = spacy.load('en_core_web_sm')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work with stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh1cTaEhS0aP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **2. Global variables (control)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW0G_4k17U18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30\n",
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_lenght = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "training_portion = .8\n",
        "\n",
        "PATH_FILE = 'data/training.1600000.processed.noemoticon.csv'\n",
        "FILE = 'data/sentiment-subset.csv'\n",
        "main_labels = ['happy', 'confident', 'sad', 'not confident']"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrPWPhX9TDs4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. **Lemmatization**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npocB7yHDfHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lemmatizion\n",
        "# stanza\n",
        "def stanza_lemma(text):\n",
        "    doc = stNLP(text)\n",
        "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpMteO-TKh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dZXQ6kgTLs8",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **3. Preprocessing & load dataset**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhZ1AG0GTZxM",
        "colab_type": "text"
      },
      "source": [
        "### i. Load database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G21PzPJFYEAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path=None):\n",
        "    !mkdir -p data\n",
        "    !wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "    !unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny2Bv6oWTkzE",
        "colab_type": "text"
      },
      "source": [
        "### ii. Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wO_zeUcFHUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_dataset():\n",
        "    # load_data\n",
        "    load_data()\n",
        "    print('Database loaded\\n')\n",
        "\n",
        "    # cleaning data\n",
        "    unclean_df = pd.read_csv(PATH_FILE,\n",
        "                     names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                     encoding='latin-1') # if utf-8: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 232719-232720: invalid continuation byte\n",
        "\n",
        "    unclean_df.polarity = unclean_df.polarity.replace({0: 0, 4: 1}) # replace polarity\n",
        "    unclean_df = unclean_df.drop(columns=['id', 'date', 'query', 'user']) # dropping unneeded columns\n",
        "\n",
        "    # sample\n",
        "    #df_sample = unclean_df.sample(n=500000)\n",
        "    #df_sample.polarity.value_counts()\n",
        "\n",
        "    # lower case\n",
        "    unclean_df['text'] = unclean_df['text'].str.lower()\n",
        "\n",
        "    # remove character and numbers\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'https://www\\.|http:\\.|https://|www\\.', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil|cl)[\\S]*\\s?', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-zÁ-Úá-ú \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?%', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "\n",
        "    # rewritting the created file without NaN values\n",
        "    unclean_df.to_csv('data/sentiment140-subset.csv', \n",
        "              quotechar='\"', # check later!\n",
        "              encoding='latin-1',\n",
        "              index=False)\n",
        "\n",
        "    # clean csv\n",
        "    df = pd.read_csv('data/sentiment140-subset.csv', encoding='latin-1').dropna()\n",
        "\n",
        "    # checking if there's any NaN values\n",
        "    isnull = [i for i in (df['text'].isnull()) if i == True]\n",
        "    if isnull != []:\n",
        "        sys.exit(0) # add response object here\n",
        "\n",
        "    \n",
        "    # STOPWORDS\n",
        "    # Getting in a list all the stopwords of the dataframe\n",
        "    #spacy_stop_words = list(dict.fromkeys([str(i) for i in spNLP(' '.join(j for j in df['text'])) if i.is_stop == True]))\n",
        "    \n",
        "    '''\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.extend(spacy_stop_words)\n",
        "    stop_words = set(stop_words)\n",
        "    \n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i not in stop_words]))\n",
        "    \n",
        "    # Lemmatization Stanza vs NLTK\n",
        "    df['text'] = df['text'].apply(lambda x: stanza_lemma(x))\n",
        "    df['text'] = df['text'].apply(lambda x: nltk_lemma(x))\n",
        "\n",
        "    # check new stopwords here!\n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join(\n",
        "        [i for i in x.split() if i not in stop_words]\n",
        "    ))\n",
        "    '''\n",
        "    # return df\n",
        "    x, y = df.text, df.polarity\n",
        "    return x, y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtGPDu4aFxQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e464e4e4-13ce-421c-9ec6-315fe30266b1"
      },
      "source": [
        "# test peprocess_dataset\n",
        "x, y = preprocess_dataset()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "Database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJqblmY3KNJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "f7980c86-2393-4ffc-c694-beb65561b45a"
      },
      "source": [
        "# test peprocess_dataset\n",
        "print(y.value_counts())\n",
        "y.value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    799982\n",
            "1    799969\n",
            "Name: polarity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQHKelS39Zan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f67a6729-2bb7-4a04-e466-8273e91bd73b"
      },
      "source": [
        "# test peprocess_dataset\n",
        "print(Counter(x).most_common(10))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(' ', 2301), (' thanks ', 1572), (' get  followers a day using once you add everyone you are on the train or pay vip ', 1324), (' thank you ', 998), (' i am lost please help me find a good home ', 507), (' me too ', 453), ('good morning ', 444), (' good morning ', 355), (' i know ', 342), ('  ', 310)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t8COv8bTslN",
        "colab_type": "text"
      },
      "source": [
        "### iii. Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci01PpVNYdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(test_size=0.2, validation_size=0.2):\n",
        "    # load dataset\n",
        "    # split dataset (as string into panda.core.series.Serie object)\n",
        "    (x, y) = preprocess_dataset()\n",
        "    \n",
        "    # create/split train, validation and test\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
        "    x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # pandas.core.series.Series to numpy array\n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "    x_validation, y_validation =  np.array(x_validation), np.array(y_validation)\n",
        "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
        "\n",
        "    return (x_train, y_train), (x_validation, y_validation), (x_test, y_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKY4MEpMiQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ed7ed23a-9b7b-4a3e-81b9-22c09cf5b9df"
      },
      "source": [
        "# test prepare_dataset function\n",
        "(x_train, y_train), (x_validation, y_validation), (x_test, y_test) = prepare_dataset()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "Database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvZkUZ5BO-n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def prepare_dataset_testing():\n",
        "    \n",
        "    # test with csv module\n",
        "    labels, texts = [], [] \n",
        "    with open('data/sentiment140-subset.csv', 'r', encoding='latin-1') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            labels.append(row[0])\n",
        "            text = row[1]\n",
        "            for i in STOPWORDS:\n",
        "                token = ' ' + i + ' '\n",
        "                text = text.replace(token, ' ')\n",
        "                text = text.replace(' ', ' ')\n",
        "            texts.append(text)\n",
        "\n",
        "    # split\n",
        "    train_size = int(len(texts) * training_portion)\n",
        "\n",
        "    train_texts = texts[:train_size]\n",
        "    train_labels = labels[:train_size]\n",
        "\n",
        "    validation_texts = texts[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok, \n",
        "                          filter='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
        "\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # text to sequences: triain_texts\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "    # padding and truncating sequences: train_seq\n",
        "    train_padded = pad_sequences(sequences=train_sequences, maxlen=max_lenght,\n",
        "                                 padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # same process: validation_texts\n",
        "    valdiation_sequences = tokenizer.texts_to_sequences(validation_texts)\n",
        "    validation_padded = pad_sequences(valdiation_sequences, maxlen=max_lenght, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # tokenize & sequences to train and validation LABELS\n",
        "    label_tokenizer = Tokenizer()\n",
        "    label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "    training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "    validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "    print(label_tokenizer.word_index)\n",
        "\n",
        "    return train_padded, training_label_seq, validation_padded, validation_label_seq"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fet5offGT4gM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **4. Build model**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iasm0J2eYsvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(learning_rate=0.0001, opt='adam', loss='categorical_crossentropy'):\n",
        "    model = Sequential()\n",
        "\n",
        "    # layers\n",
        "    model.add(Embedding(vocab_size, embedding_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(embedding_dim)))\n",
        "\n",
        "    # softmax output layer\n",
        "    model.add(tf.keras.layers.Dense(units=2, # 10 \n",
        "                                    activation='softmax'))\n",
        "\n",
        "    # optimizer & loss\n",
        "    opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss='sparse_categorical_crossentropy'\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss,\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35NcHgPNUHPt",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **5. Train model** \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OaqUi0_ZwH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, x_train, y_train, x_validation, y_validation,\n",
        "          epochs, batch_size=32, patience=5, \n",
        "          verbose=2, monitor='accuracy'):\n",
        "    \n",
        "    # callback\n",
        "    early_callback = tf.keras.callbacks.EarlyStopping(monitor=monitor, # also try 'val_loss'\n",
        "                                                      verbose=1, mode='auto', restore_best_weights=True,\n",
        "                                                      min_delta=1e-3, patience=patience)\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                        validation_data=(x_validation, y_validation), # x_test, y_test\n",
        "                        callbacks=[early_callback])\n",
        "    return history"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9UK7KmKUOQV",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **6. Plotting history**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsuytCdNDyDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history_(history):\n",
        "    fitModel_dict = history.history\n",
        "    acc = fitModel_dict['accuracy']\n",
        "    val_acc = fitModel_dict['val_accuracy']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.ylim((0.5, 1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_history(history, string):\n",
        "    fitModel_dict = history.history\n",
        "    plt.plot(fitModel_dict[string])\n",
        "    plt.plot/fitModel_dict['val_' + string]\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_' + string])\n",
        "    plt.show()\n",
        "\n",
        "# test\n",
        "#plot_history(history, 'accuracy')\n",
        "#plot_history(history, 'loss')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2lMVAEFUVj9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **7. Main**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvUnHHkbykJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # x_train, y_train, x_validation, y_validation, x_test, y_test = prepare_dataset(data_path) # define data path for google colab and visual studio with statements\n",
        "    train_padded, training_label_seq, validation_padded, validation_label_seq = prepare_dataset_testing()\n",
        "\n",
        "    # input_shape\n",
        "    model = build_model()\n",
        "\n",
        "    history = train(model=model, x_train=train_padded, y_train=training_label_seq,\n",
        "                    x_validation=validation_padded, y_validation=validation_labels,\n",
        "                    epochs=EPOCHS, verbose=1)\n",
        "    plot_history(history)\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    print('\\nTest:\\nLoss: {}\\nAccuracy: {}').format(loss, accuracy * 100)\n",
        "\n",
        "    model.save(model.h5)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Xg-eujUahT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **8. __name__ == \"__main__\"**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBushApRdFKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}