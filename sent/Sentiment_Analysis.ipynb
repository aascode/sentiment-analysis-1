{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKssXJ-F5FGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e3c5128-24f3-4315-e998-4de7a50ae07b"
      },
      "source": [
        "import logging\n",
        "\n",
        "tf_response = {\n",
        "    'error': None,\n",
        "    'version':tf.__version__\n",
        "}\n",
        "\n",
        "try:\n",
        "    # specifying the TensorFlow version to google colab\n",
        "    %tensorflow_version 2.x \n",
        "except OSError as error:\n",
        "    response['error'] = logging.debug('You are not using your specify version of TensorFlow')\n",
        "finally:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "print(tf_response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'error': None, 'version': '2.3.0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW1C6W9DQws8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd # to dataset (csv) manipulation\n",
        "import numpy as np # to array\n",
        "from collections import Counter\n",
        "import re # regular expressions\n",
        "import matplotlib.pyplot as plt # to create graphs\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBIi3kzrV3ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "da5690a3-a718-42a4-8cf3-bf6ed89cfbca"
      },
      "source": [
        "# Natural Language Preprocessing\n",
        "# Stopwords\n",
        "import nltk # Natural Language ToolKit\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # to get rid of StopWords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sweo4LneWAfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "2281a3d8-54df-41c8-96ef-25653ae1d4da"
      },
      "source": [
        "# Stanza NLP\n",
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (49.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 16.4MB/s]                    \n",
            "2020-08-20 00:54:21 WARNING: Can not find mwt: ewt from official model list. Ignoring it.\n",
            "2020-08-20 00:54:21 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/tokenize/ewt.pt: 100%|██████████| 631k/631k [00:00<00:00, 1.81MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/pos/ewt.pt: 100%|██████████| 22.1M/22.1M [00:00<00:00, 26.4MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/lemma/ewt.pt: 100%|██████████| 3.36M/3.36M [00:00<00:00, 5.99MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/pretrain/ewt.pt: 100%|██████████| 156M/156M [00:13<00:00, 11.4MB/s]\n",
            "2020-08-20 00:54:39 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-08-20 00:54:39 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2020-08-20 00:54:39 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-08-20 00:54:39 INFO: Use device: gpu\n",
            "2020-08-20 00:54:39 INFO: Loading: tokenize\n",
            "2020-08-20 00:54:49 INFO: Loading: pos\n",
            "2020-08-20 00:54:50 INFO: Loading: lemma\n",
            "2020-08-20 00:54:50 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zH74LpiW1aM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "cfa6e422-183c-4595-b0ff-a71ae5e15897"
      },
      "source": [
        "doc = stNLP('Barack Obama nació en Hawaii.')\n",
        "\n",
        "print('\\n')\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: nació \tlemma: nació\n",
            "word: en \tlemma: en\n",
            "word: Hawaii \tlemma: Hawaii\n",
            "word: . \tlemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G21PzPJFYEAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path):\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci01PpVNYdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(data_path, test_size=0.2, validation_size=0.2):\n",
        "    return x_train, y_train, x_validation, y_validation "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iasm0J2eYsvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape, loss='sparse_categorical_crossentropy', learning_rate=0.0001):\n",
        "    model = tf.keras.layers.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss,\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OaqUi0_ZwH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, batch_size, epochs, patience, x_train, y_train, x_validation, y_validation, verbose=2):\n",
        "    # callback\n",
        "    early_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.001, patience=patience)\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=verbose # default 1\n",
        "                        validation_data=(x_validation, y_validation),\n",
        "                        callbacks=[early_callback])\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDxJsWKabpV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvUnHHkbykJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    x_train, y_train, x_validation, y_validation, x_test, y_test = prepare_dataset(data_path) # define data path for google colab and visual studio with statements\n",
        "\n",
        "    # input_shape\n",
        "    # model = build_model()\n",
        "\n",
        "    # history = train(model)\n",
        "    # plot_history(history)\n",
        "\n",
        "    # loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    # print('\\n Loss: {}\\nAccuracy: {}').format(loss, accuracy * 100)\n",
        "\n",
        "    # model.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBushApRdFKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}